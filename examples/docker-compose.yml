# Docker Compose example for legible with Ollama
#
# This runs legible in daemon mode with Ollama for OCR processing.
#
# Usage:
#   1. First, authenticate:
#      docker-compose run --rm legible auth
#
#   2. Start the daemon:
#      docker-compose up -d
#
#   3. View logs:
#      docker-compose logs -f
#
#   4. Stop the daemon:
#      docker-compose down
#
# System Requirements:
#   - 10GB+ disk space (mistral-small3.1 model is ~7-8GB)
#   - 6GB+ RAM recommended (for mistral-small3.1)
#   - CPU with decent performance for OCR

version: '3.8'

services:
  legible:
    image: ghcr.io/platinummonkey/legible:latest
    container_name: legible
    restart: unless-stopped

    # Volumes for persistent data
    volumes:
      # reMarkable API credentials (created during auth)
      - ./credentials:/home/remarkable/.rmapi

      # Output directory for synced documents
      - ./output:/output

      # Ollama models cache (persists downloaded models)
      - ollama-models:/home/remarkable/.ollama/models

    # Environment variables for Ollama configuration
    environment:
      # Ollama service endpoint (internal)
      - OLLAMA_HOST=http://localhost:11434

      # OCR model to use (mistral-small3.1, llava, llava:13b, etc.)
      # Recommended models:
      # - mistral-small3.1: Better multilingual and complex handwriting (~7-8GB) [DEFAULT]
      # - llava: Faster, good for most handwriting (~4GB)
      # - llava:13b: Higher accuracy, slower (~7GB)
      - OCR_MODEL=mistral-small3.1

      # Optional: Increase if running large models
      # - OLLAMA_NUM_PARALLEL=1
      # - OLLAMA_MAX_LOADED_MODELS=1

    # Run in daemon mode, syncing every hour
    command: >
      daemon
      --interval 1h
      --output /output
      --log-level info

    # Health check for both Ollama and legible
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags && pgrep legible"]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 30s

    # Resource limits (adjust based on your model and usage)
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 6G  # Increased for mistral-small3.1 model
        reservations:
          cpus: '0.5'
          memory: 2G

volumes:
  # Persistent volume for Ollama models
  ollama-models:
    driver: local

# Example: Using an external Ollama service
#
# If you want to run Ollama separately (e.g., on a different machine
# or shared across containers), you can use this configuration:
#
# services:
#   ollama:
#     image: ollama/ollama:latest
#     container_name: ollama
#     restart: unless-stopped
#     volumes:
#       - ollama-models:/root/.ollama
#     ports:
#       - "11434:11434"
#
#   legible:
#     image: ghcr.io/platinummonkey/legible:latest
#     depends_on:
#       - ollama
#     environment:
#       - OLLAMA_HOST=http://ollama:11434
#       - OCR_MODEL=llava
#     volumes:
#       - ./credentials:/home/remarkable/.rmapi
#       - ./output:/output
#     command: >
#       daemon
#       --interval 1h
#       --output /output

# Example: Using mistral-small3.1 for better multilingual OCR
#
# Mistral Small 3.1 provides better accuracy for:
# - Multilingual handwriting
# - Complex or messy handwriting
# - Mixed language documents
#
# Note: Larger model (~7-8GB) requires more RAM and takes longer per page
#
# services:
#   legible:
#     image: ghcr.io/platinummonkey/legible:latest
#     environment:
#       - OCR_MODEL=mistral-small3.1
#     volumes:
#       - ./credentials:/home/remarkable/.rmapi
#       - ./output:/output
#       - ollama-models:/home/remarkable/.ollama/models
#     deploy:
#       resources:
#         limits:
#           memory: 6G  # Increase memory for larger model
#     command: >
#       daemon
#       --interval 1h
#       --output /output

# Example: Using host-mounted Ollama models
#
# Mount Ollama models from your host machine to avoid re-downloading
# This is useful if you already have models downloaded or want to share
# models across multiple containers.
#
# First, download the model on your host:
#   ollama pull mistral-small3.1
#
# Then mount your host's Ollama models directory:
#
# services:
#   legible:
#     image: ghcr.io/platinummonkey/legible:latest
#     environment:
#       - OCR_MODEL=mistral-small3.1
#     volumes:
#       - ./credentials:/home/remarkable/.rmapi
#       - ./output:/output
#       # Mount host Ollama models (adjust path for your OS)
#       # macOS/Linux: ~/.ollama/models
#       # Windows: %USERPROFILE%\.ollama\models
#       - ~/.ollama/models:/home/remarkable/.ollama/models:ro  # :ro = read-only
#     command: >
#       daemon
#       --interval 1h
#       --output /output

# Example: With labels filter
#
# To sync only documents with specific labels:
#
# services:
#   legible:
#     image: ghcr.io/platinummonkey/legible:latest
#     environment:
#       - OCR_MODEL=llava
#     volumes:
#       - ./credentials:/home/remarkable/.rmapi
#       - ./output:/output
#       - ollama-models:/home/remarkable/.ollama/models
#     command: >
#       daemon
#       --interval 1h
#       --output /output
#       --labels "work,personal"

# Example: Development setup with local build
#
# To build and run locally for testing:
#
# services:
#   legible:
#     build:
#       context: ..
#       dockerfile: Dockerfile
#     environment:
#       - OCR_MODEL=llava
#     volumes:
#       - ./credentials:/home/remarkable/.rmapi
#       - ./output:/output
#       - ollama-models:/home/remarkable/.ollama/models
#     command: >
#       daemon
#       --interval 1h
#       --output /output
#       --log-level debug
